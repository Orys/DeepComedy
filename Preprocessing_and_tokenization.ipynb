{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Preprocessing_and_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoCozzi96/DeepComedy/blob/main/Preprocessing_and_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPi_U9KYU_TO"
      },
      "source": [
        "# Preprocessing and Tokenization\n",
        "This notebook is used to create the tokenized dataset which will later be used to train the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik0S5tlsMUge"
      },
      "source": [
        "We have stored our parsed and tokenized datasets in our GitHub repository, which can be cloned. The parsed (clean) datasets and the tokenized texts are already available in the \"DeepComedy/dataset\" folder but by running the cells below they will be overwritten by the new dataset created by the new code. By default, this notebook creates the exact datasets we provided. \n",
        "\n",
        "For a better explaination of what is happening here, please read our relation on GitHub [here](https://github.com/RiccardoCozzi96/DeepComedy/blob/main/Cozzi-Liscio%20(2020)%20Deep%20Comedy%20project%20work%20report.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PEZql5oMUgh",
        "outputId": "b964fc13-5871-4606-ae4b-a93020449f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "import io\n",
        "import sys\n",
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd\n",
        "!pip install pyphen\n",
        "\n",
        "# retrieve our GitHub repository\n",
        "!git clone \"https://github.com/RiccardoCozzi96/DeepComedy\"\n",
        "\n",
        "sys.path.append(\"DeepComedy/tokenizer/\")\n",
        "sys.path.append(\"DeepComedy/metrics/\")\n",
        "sys.path.append(\"DeepComedy/datasets/\")\n",
        "\n",
        "from comedy_tokenizer import ComedyTokenizer\n",
        "from comedy_metrics import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen\n",
            "Successfully installed pyphen-0.10.0\n",
            "Cloning into 'DeepComedy'...\n",
            "remote: Enumerating objects: 421, done.\u001b[K\n",
            "remote: Total 421 (delta 0), reused 0 (delta 0), pack-reused 421\n",
            "Receiving objects: 100% (421/421), 1.67 MiB | 1.84 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loqpt4GSS0VS"
      },
      "source": [
        "# settings folders and files names\n",
        "dataset_folder = \"DeepComedy/datasets/\"\n",
        "parsed_folder = dataset_folder+\"parsed/\"\n",
        "tokenized_folder = dataset_folder+\"tokenized/\"\n",
        "original_text_filename = [\"commedia\", \"convivio\", \"detto\", \"vita\", \"fiore\"]\n",
        "parsed_text_filename = \"parsed_*.txt\"\n",
        "tokenized_text_filename = \"tokenized_*.txt\"\n",
        "hyphenation_dictionary = \"DeepComedy/tokenizer/dantes_hyphenation_dictionary.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8h5KiGlWKAe"
      },
      "source": [
        "## Hyphenation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZbK0H2tMUgw"
      },
      "source": [
        "### Uploading the hyphenation dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWJ5PaLoMUgx",
        "outputId": "7bc501ea-00fc-43f5-f008-581a9832061c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "# load the DANTE'S dictionary from file\n",
        "hyphenation_vocabulary = pd.read_csv(hyphenation_dictionary, index_col=\"word\")\n",
        "hyphenation_vocabulary = hyphenation_vocabulary.iloc[:, :1] # the 2rd col contains the tunes, not relevant for hyphenation\n",
        "hyphenation_vocabulary.head(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyphenation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abaglia</th>\n",
              "      <td>a-ba-glia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abaier</th>\n",
              "      <td>a-ba-ier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandonarmi</th>\n",
              "      <td>a-ban-do-nar-mi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandonato</th>\n",
              "      <td>a-ban-do-na-to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandono</th>\n",
              "      <td>a-ban-dó-no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zita</th>\n",
              "      <td>zì-ta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zodiaco</th>\n",
              "      <td>zo-dì-a-co</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zona</th>\n",
              "      <td>zò-na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zucca</th>\n",
              "      <td>zùc-ca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zuffa</th>\n",
              "      <td>zùf-fa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16979 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 hyphenation\n",
              "word                        \n",
              "abaglia            a-ba-glia\n",
              "abaier              a-ba-ier\n",
              "abandonarmi  a-ban-do-nar-mi\n",
              "abandonato    a-ban-do-na-to\n",
              "abandono         a-ban-dó-no\n",
              "...                      ...\n",
              "zita                   zì-ta\n",
              "zodiaco           zo-dì-a-co\n",
              "zona                   zò-na\n",
              "zucca                 zùc-ca\n",
              "zuffa                 zùf-fa\n",
              "\n",
              "[16979 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-j_z_KpMUg3",
        "outputId": "3d538fdd-b168-4b29-d526-dd0825d81146",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# transforming to dictionary\n",
        "hyphenation_dictionary = hyphenation_vocabulary.to_dict()[\"hyphenation\"]\n",
        "\n",
        "#some words have accents\n",
        "print(hyphenation_dictionary[\"zavorra\"])\n",
        "\n",
        "#some others don't\n",
        "print(hyphenation_dictionary[\"oscura\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "za-vòr-ra\n",
            "o-scu-ra\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMD21oqEMUhB"
      },
      "source": [
        "### Creating tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX53tAo0Wj_G"
      },
      "source": [
        "The class Tokenizer provides all the methods needed to hyphenate and tokenize a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72egrxgEMUhC",
        "outputId": "9b513555-49ad-478b-a7cf-6f58561c5db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pyphen\n",
        "from comedy_tokenizer import ComedyTokenizer\n",
        "\n",
        "tokenizer = ComedyTokenizer(dictionary=hyphenation_dictionary, \n",
        "                            synalepha=True, \n",
        "                            use_tercets=True)\n",
        "\n",
        "# alternatively, load the csv file one in this line\n",
        "# tokenizer = ComedyTokenizer.from_dataframe(pd.read_csv(\"ultimate_hyphenation.csv\", index_col=\"word\"),\n",
        "#                                            synalepha=True, \n",
        "#                                            use_tercets=True)\n",
        "print(tokenizer.hyphenate(\"zavorra\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "za-vòr-ra\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fpkCVLAMUhI"
      },
      "source": [
        "### Testing hyphenation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3HK8YuKW1h7"
      },
      "source": [
        "Our tokenizer exploits both `pyphen` procedure for hyphenation and the .csv file built by hyphenating all the Dante's terms (in Divine Comedy and other productions). Pyphen is used in exceptions cases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u015F3qfMUhJ",
        "outputId": "afe1225c-830a-4c5c-aa20-1fdc84fffb71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### HYPHENATION TEST ###\n",
        "import pyphen\n",
        "import pandas as pd\n",
        "dic = pyphen.Pyphen(lang='it')\n",
        "errors = \"oscura atletica ostracismo cruento paura aiuta diocesi odracardo anima\".split(\" \")\n",
        "print(\"{:20}  {:20} {:30}\".format(\"word\", \"pyphen\", \"our\"))\n",
        "print(\"-\"*80)\n",
        "\n",
        "\n",
        "\n",
        "for e in errors:\n",
        "    print(\"{:20}  {:20} {:30}\".format(e, dic.inserted(e), tokenizer.hyphenate(e)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word                  pyphen               our                           \n",
            "--------------------------------------------------------------------------------\n",
            "oscura                oscu-ra              o-scu-ra                      \n",
            "atletica              atle-ti-ca           atle-ti-ca                    \n",
            "ostracismo            ostra-ci-smo         ostra-ci-smo                  \n",
            "cruento               cruen-to             cruen-to                      \n",
            "paura                 pau-ra               pa-ù-ra                       \n",
            "aiuta                 aiu-ta               a-iu-ta                       \n",
            "diocesi               dio-ce-si            dio-ce-si                     \n",
            "odracardo             odra-car-do          odra-car-do                   \n",
            "anima                 ani-ma               à-ni-ma                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvx75TiMUhS"
      },
      "source": [
        "### Testing hyphenation and synalepha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO6vIBofXS1t"
      },
      "source": [
        "Each word is splitted in syllables divided by spaces. Then the text is tagged as follows: \n",
        "* spaces: `<S>`\n",
        "\n",
        "* start of verse:`<V>`\n",
        "* end of verse: `</V>`\n",
        "* end of verse: `</V>`\n",
        "* start of tercet: `<T>`\n",
        "* <S>end of tercet: `</T>`</S> *(seems to be not useful)*\n",
        "* synalepha (between tokens A and B): `A~B`\n",
        "\n",
        "**NOTE**: the accented characters are retrieved by the hyphenation dictionary. If needed, they can be easily replaced by their coresponding unaccented one when not at the ending of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Jmf1NJJPMUhT",
        "outputId": "41bf9c5a-10a4-4f46-a80a-efe45b19e313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = [\"e tu che se' costì anima viva\",\n",
        "        \"nel mezzo del cammin di nostra vita\",\n",
        "        \"mi ritrovai per una selva oscura\", \n",
        "        \"che la diritta via era smarrita\",\n",
        "        \"a un pianto o a un riso\",\n",
        "        \"selvaggia e aspra e forte\"]\n",
        "\n",
        "for t in test:\n",
        "    s = tokenizer.tokenize_phrase(t)\n",
        "    print(\"\\n{:40}\\n{:40}\".format(s, tokenizer.clear_text(s)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<V> e <S> tu <S> ché <S> se' <S> co stì <S> à ni ma <S> vì va </V>\n",
            "e tu ché se' costì ànima vìva\n",
            "          \n",
            "\n",
            "<V> nél <S> mèz zo <S> dél <S> cam min <S> di <S> no stra <S> vì ta </V>\n",
            "nél mèzzo dél cammin di nostra vìta\n",
            "    \n",
            "\n",
            "<V> mi <S> ri tro va i <S> per <S> ù na <S> sél va~o scu ra </V>\n",
            "mi ritrovai per ùna sélva oscura\n",
            "       \n",
            "\n",
            "<V> ché <S> la <S> di rìt ta <S> vì a <S> è ra <S> smar ri ta </V>\n",
            "ché la dirìtta vìa èra smarrita\n",
            "        \n",
            "\n",
            "<V> a~un <S> piàn to~o~a~un <S> rì so </V>\n",
            "a un piànto o a un rìso\n",
            "                \n",
            "\n",
            "<V> sel vag gia~e <S> à spra~e <S> fòr te </V>\n",
            "selvaggia e àspra e fòrte\n",
            "              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma1UKLIyMUho"
      },
      "source": [
        "## Tokenizing Dante's productions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl-2YmblY7BH"
      },
      "source": [
        "Now let us tokenize all the datasets, starting from the Divine Comedy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNR-tpseMUhp",
        "outputId": "94118161-e7af-494f-98d1-e5da0639a311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for file_name in original_text_filename:\n",
        "    \n",
        "    # tokenize texts\n",
        "    parsed_path = parsed_folder + parsed_text_filename.replace(\"*\", file_name)\n",
        "    with open(parsed_path, encoding=\"utf-8\") as file:\n",
        "        data = file.readlines()\n",
        "        data = tokenizer.tokenize_text(data, use_tercets = True if file_name == \"commedia\" else False)\n",
        "        if file_name == \"commedia\":\n",
        "            print(data[:6], \"\\n\")\n",
        "        \n",
        "    # write tokenized text to file\n",
        "    tokenized_path = tokenized_folder + tokenized_text_filename.replace(\"*\", file_name)\n",
        "    with open(tokenized_path, \"w+\", encoding=\"utf-8\") as out:\n",
        "        for line in data:\n",
        "            out.write(line+\"\\n\")\n",
        "   \n",
        "    print(f\"'{file_name}' tokenized and saved as {tokenized_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<T> <V> nél <S> mèz zo <S> dél <S> cam min <S> di <S> no stra <S> vì ta </V>'\n",
            " '<V> mi <S> ri tro va i <S> per <S> ù na <S> sél va~o scu ra </V>'\n",
            " \"<V> che' <S> la <S> di rìt ta <S> vì a <S> è ra <S> smar ri ta . </V>\"\n",
            " '<T> <V> à hi <S> quàn to~a <S> dir <S> qual <S> è ra <S> è <S> cò sa <S> dù ra </V>'\n",
            " '<V> é sta <S> sél va <S> sel vag gia~e <S> à spra~e <S> fòr te </V>'\n",
            " '<V> ché <S> nél <S> pen sier <S> ri no va <S> la <S> pau ra ! </V>'] \n",
            "\n",
            "'commedia' tokenized and saved as DeepComedy/datasets/tokenized/tokenized_commedia.txt\n",
            "'convivio' tokenized and saved as DeepComedy/datasets/tokenized/tokenized_convivio.txt\n",
            "'detto' tokenized and saved as DeepComedy/datasets/tokenized/tokenized_detto.txt\n",
            "'vita' tokenized and saved as DeepComedy/datasets/tokenized/tokenized_vita.txt\n",
            "'fiore' tokenized and saved as DeepComedy/datasets/tokenized/tokenized_fiore.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}